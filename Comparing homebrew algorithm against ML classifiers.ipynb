{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I do a quick exploration on a hand-labeled dataset of 500 reviews, which have information on whether or not any side effect was mentioned, to see if standard classifiers perform better than my homebrewed algorithm at finding side effects. I did not compare on the multilabel problem of identifying specific side effects, because the burden of labeling that dataset was too high for the ROI. I did, however, spend some time looking at the feature importances for the logistic regression model, and found that many of the features that positively correlated were what you might expect (the words \"weight\" and \"diarrhea\", negative sentiment, etc.) and the same was true for what was negatively correlated (positive sentiment, etc.). This would have been an interesting avenue to chase down, but having verified that my algorithm worked well, I decided to spend more time tuning the parameters on that, rather than exploring the classification problem.\n",
    "\n",
    "It should be noted that this notebook is more of a scratch pad, and most of the code here that is undocumented and minimally commented is commented and documented in the .py files, which are what I used in my final analysis (i.e., what I consider to be the \"production\" code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob as glob\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()\n",
    "\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from autocorrect import Speller\n",
    "spell = Speller(lang='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for my homebrew method (implementation and grid search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import vader\n",
    "VADER_SIA = vader.SentimentIntensityAnalyzer()\n",
    "\n",
    "def find_polarity_scores(reviews):\n",
    "    VADERscorePos = []\n",
    "    VADERscoreNeg = []\n",
    "    for rev in reviews:\n",
    "        VADERscorePos.append(VADER_SIA.polarity_scores(rev)['pos'])    \n",
    "        VADERscoreNeg.append(VADER_SIA.polarity_scores(rev)['neg'])            \n",
    "        \n",
    "    return VADERscorePos, VADERscoreNeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic tokenizer thing\n",
    "def spacyTokenizer(s: str)-> list:\n",
    "    doc = tokenizer(s.lower().strip())\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if not token.is_stop and token.is_alpha and token.lemma_ != '-PRON-':\n",
    "            tokens.append(token.lemma_)\n",
    "        \n",
    "    return tokens\n",
    "\n",
    "# Doing the tf-idf bit\n",
    "def findSEFeatures(strList):\n",
    "    tfidf_vectr = TfidfVectorizer()\n",
    "    corpus = [' '.join(SE) for SE in strList]\n",
    "    tfidf_score = tfidf_vectr.fit_transform(corpus)\n",
    "    features = np.array(tfidf_vectr.get_feature_names())\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Using the side effect features list as the vocabulary for the reviews, \n",
    "# as a backhanded way of pulling out features\n",
    "def findTFIDFReviews(reviews, SEvocab):\n",
    "    tfidf_vectr = TfidfVectorizer(vocabulary=SEvocab)\n",
    "    corpus = [' '.join(rev) for rev in reviews]\n",
    "   \n",
    "    tfidf_score = tfidf_vectr.fit_transform(corpus).toarray()\n",
    "\n",
    "    return tfidf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processLabeledReviews(SEvocab, ReviewFile='LabeledReviews/randomlySelectedReviews.csv'):\n",
    "    df = pd.read_csv(ReviewFile, sep='$', index_col=0)\n",
    "    \n",
    "    # Counting medication mentions as a feature\n",
    "    medications = np.unique(df['Medication'])\n",
    "    medications = np.array([med.lower().replace('-',' ') for med in medications])\n",
    "    med_counts = []\n",
    "    for rev in df['Full Review']:\n",
    "        split_rev = rev.split(' ')\n",
    "        matches = []\n",
    "        for med in medications:\n",
    "            medSplit = list(med)\n",
    "            lenMed = len(medSplit)\n",
    "            for word in split_rev:\n",
    "                wordSplit = list(word.lower())\n",
    "                ind = min([lenMed, len(wordSplit)])\n",
    "                test = ''.join([w for i,w in enumerate(wordSplit[:ind]) if w == medSplit[i] ])\n",
    "                if len(test) >= max([ind,len(medSplit)-2]):\n",
    "                    matches.append(med)\n",
    "        med_counts.append(np.unique(matches).size)\n",
    "\n",
    "    df['Medication mentions'] = med_counts\n",
    "    \n",
    "    reviews = np.array(df['Full Review'])\n",
    "    pos, neg = find_polarity_scores(reviews)\n",
    "    reviews = [[spell(word) for word in spacyTokenizer(rev.replace('/', ' '))] for rev in reviews]\n",
    "    \n",
    "    features = findTFIDFReviews(reviews, SEvocab)\n",
    "    stack = np.array([df['Medication mentions'], pos, neg]+[feat for feat in features.T]+[df['Presence of side effect']]).T\n",
    "    \n",
    "    newDF = pd.DataFrame(stack, columns = ['Medication mentions', 'Positive', 'Negative']+list(SEvocab)+['Side effect label'])\n",
    "\n",
    "    return newDF\n",
    "\n",
    "# Need to collect FAERs results and basic Drugs.com info\n",
    "def processSideEffects(medFile='MedicationsAndSideEffects.csv',\n",
    "                       conditions=['ADHD', 'Anxiety', 'Bipolar-Disorder', 'Depression', 'Schizophrenia']):\n",
    "    \n",
    "    # Processing the Drugs.com side effects\n",
    "    df = pd.read_csv(medFile, sep='$', index_col=0)\n",
    "    readLine = lambda s: s[2:-2].split('; ')[:-1]\n",
    "    allSEs = []\n",
    "    for key in ['More common', 'Less common', 'Incidence not known']:\n",
    "        for item in [readLine(SE) for SE in df[key]]: allSEs += item\n",
    "    allSEs = list(np.unique(allSEs))\n",
    "    \n",
    "    allSEs = [[spell(word) for word in spacyTokenizer(SE)] for SE in allSEs]\n",
    "    \n",
    "    \n",
    "    # Handling the FAERs side effects\n",
    "    moreSEs = []\n",
    "    for condition in conditions:\n",
    "        mfile = pd.read_csv('faers_results/{:s}/SideEffectsExtracted.csv'.format(condition),\n",
    "                           sep='$', index_col=0)\n",
    "        mfile = mfile.fillna('')\n",
    "        moreSEs += [mfile.loc[ind]['Definition']+mfile.loc[ind]['Synonyms'] for ind in mfile.index]\n",
    "    moreSEs = list(np.unique(moreSEs))\n",
    "    moreSEs = [[spell(word) for word in spacyTokenizer(SE)] for SE in moreSEs]\n",
    "\n",
    "    return allSEs+moreSEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not super useful to take the mean across columns, instead look at top 10 scoring words in each side effect\n",
    "def findTop(strList, keeptop=10, extracut=5, topcutoff=0.0099):\n",
    "    tfidf_vectr = TfidfVectorizer()\n",
    "    corpus = [' '.join(SE) for SE in strList]\n",
    "    tfidf_score = tfidf_vectr.fit_transform(corpus).toarray()\n",
    "    features = np.array(tfidf_vectr.get_feature_names())\n",
    "    \n",
    "    words = []\n",
    "    for row in tfidf_score:\n",
    "        topcut = min([keeptop,len(row)])\n",
    "        inds = row.argsort()[::-1][:topcut]\n",
    "        row_words = []\n",
    "        for ind in inds:\n",
    "            if row[ind] >= topcutoff:\n",
    "                row_words.append(features[ind])\n",
    "                #print(features[ind],' '*(50-len(features[ind])), row[ind].round(2))\n",
    "        #print('\\n')\n",
    "        if not row_words:\n",
    "            row_words = list(features[inds][:extracut])\n",
    "        words.append(row_words)\n",
    "    return words\n",
    "\n",
    "def parseRevAndSEs(SEvocab, top_cutoff=0.0099, topRev=50, topRevextra=20,topSE=5, topSEextra=5):\n",
    "    df = pd.read_csv('LabeledReviews/randomlySelectedReviews.csv', sep='$', index_col=0)\n",
    "    reviews = df['Full Review']\n",
    "\n",
    "    # Counting medication mentions as a feature\n",
    "    medications = np.unique(df['Medication'])\n",
    "    medications = np.array([med.lower().replace('-',' ') for med in medications])\n",
    "    med_counts = []\n",
    "    for rev in df['Full Review']:\n",
    "        split_rev = rev.split(' ')\n",
    "        matches = []\n",
    "        for med in medications:\n",
    "            medSplit = list(med)\n",
    "            lenMed = len(medSplit)\n",
    "            for word in split_rev:\n",
    "                wordSplit = list(word.lower())\n",
    "                ind = min([lenMed, len(wordSplit)])\n",
    "                test = ''.join([w for i,w in enumerate(wordSplit[:ind]) if w == medSplit[i] ])\n",
    "                if len(test) >= max([ind,len(medSplit)-2]):\n",
    "                    matches.append(med)\n",
    "        med_counts.append(np.unique(matches).size)\n",
    "    \n",
    "    pos, neg = find_polarity_scores(reviews)\n",
    "    reviews = [[spell(word) for word in spacyTokenizer(rev.replace('/', ' '))] for rev in reviews]\n",
    "    clean_reviews = findTop(reviews, keeptop=topRev, extracut=topRevextra)\n",
    "    \n",
    "    clean_SEs = findTop(SEvocab, keeptop=topSE, extracut=topSEextra)\n",
    "    \n",
    "    labels = df['Presence of side effect']\n",
    "    \n",
    "    return list(df['Full Review']), clean_reviews, clean_SEs, med_counts, labels\n",
    "    \n",
    "import time\n",
    "\n",
    "def find_sideEffects_inReviews_FAERsinformed(SEvocab, top_cutoff=0.0099, topRev=50, \n",
    "                                             topRevextra=20,topSE=5, topSEextra=5):\n",
    "\n",
    "    fullrevs, reviews, listSEs, med_counts, labels = parseRevAndSEs(SEvocab,top_cutoff=top_cutoff,\n",
    "                                                                   topRev=topRev, topRevextra=topRevextra,\n",
    "                                                                   topSE=topSE, topSEextra=topSEextra)\n",
    "    \n",
    "    #print(time.time())\n",
    "    BagOSE = ' '.join([' '.join(SE) for SE in listSEs])\n",
    "\n",
    "    # Finding review words that exist in the list of side effects\n",
    "    # Only requiring space at the beginning because of words like nausea-nauseated, etc.\n",
    "    found = [[word for word in rev if BagOSE.lower().find(' '+word.lower())] for rev in reviews]\n",
    "    found = []\n",
    "    for ind, rev in enumerate(reviews):\n",
    "        item = {}\n",
    "        for SE in listSEs:\n",
    "            # Match words in reviews to side effects and then add them to found, build dataframe with this info\n",
    "            item[', '.join(SE)] = len([word for word in rev if word.lower() in SE])\n",
    "        found.append(item)\n",
    "    \n",
    "    SE_match = pd.DataFrame(found)\n",
    "    SE_match['Full Review'] = fullrevs\n",
    "    SE_match['Medication mentions'] = med_counts\n",
    "    SE_match['Side effect label'] = labels\n",
    "    pos, neg = find_polarity_scores(fullrevs)\n",
    "    SE_match['Positive polarity'] = pos\n",
    "    SE_match['Negative polarity'] = neg\n",
    "    \n",
    "    # Return the master product\n",
    "    return SE_match\n",
    "\n",
    "def screen_for_hits(df, posnegRat=2.5):\n",
    "    newdf = df.drop(columns=['Full Review', 'Positive polarity', 'Negative polarity'])\n",
    "    review_inds = []\n",
    "    \n",
    "    # Allowing for two item side effects UNLESS they contain very generic words\n",
    "    colLens = np.array([len(col.split(', ')) + 2*((col.find('skin')!=-1)|\n",
    "                                                  (col.find('feel') != -1)|\n",
    "                                                  (col.find('pain')!= -1)|\n",
    "                                                  (col.find('abnormal')!=-1)|\n",
    "                                                  (col.find('change')!=-1)|\n",
    "                                                  (col.find('disorder')!=-1)|\n",
    "                                                  (col.find('problem')!=-1)|\n",
    "                                                  (col.find('decrease')!=-1)|\n",
    "                                                  (col.find('increase')!=-1)|\n",
    "                                                  (col.find('loss')!=-1)) for col in newdf.columns])\n",
    "    \n",
    "    # If the column is not generic and has two or fewer words, count one word as a match, otherwise require 2\n",
    "    for ind in newdf.index:\n",
    "        if (((colLens < 3) & newdf.loc[ind].gt(0)) | newdf.loc[ind].gt(1)).sum(): \n",
    "            review_inds.append(ind)\n",
    "            \n",
    "    # Screening based on polarity\n",
    "    cond = (df['Negative polarity'] != 0)\n",
    "    diff_inds = df.index[cond][(df['Positive polarity'][cond]/df['Negative polarity'][cond] > posnegRat)]\n",
    "    \n",
    "    # Marking hits versus not\n",
    "    found_reviews = []\n",
    "    for ind in review_inds:\n",
    "        if ind not in diff_inds and df.loc[ind]['Medication mentions'] < 2:\n",
    "            conditions = np.logical_or(np.logical_and((colLens < 3), newdf.loc[ind].gt(0)), newdf.loc[ind].gt(1))\n",
    "            hit = conditions.sum()\n",
    "            if hit:\n",
    "                found_reviews.append(ind)\n",
    "    \n",
    "    tp = df.loc[np.array(found_reviews)]['Side effect label'].sum()\n",
    "    tn = len(found_reviews) - tp\n",
    "    fp = df.drop(index=np.array(found_reviews))['Side effect label'].sum()\n",
    "    fn = len(df) - len(found_reviews) - fp\n",
    "    \n",
    "    accuracy = round((tp+tn)/len(df),2)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print(\"tn, fp, fn, tp\")\n",
    "    print(tn, fp, fn, tp)\n",
    "    #print(time.time())\n",
    "    \n",
    "    return accuracy\n",
    "    \n",
    "def mymethod(SEvocab, top_cutoff=0.005, topRev=50, topRevextra=20,topSE=5, topSEextra=5, posnegRat=2.5):\n",
    "    df = find_sideEffects_inReviews_FAERsinformed(SEvocab, top_cutoff=top_cutoff, \n",
    "                                                  topRev=topRev, topRevextra=topRevextra,\n",
    "                                                  topSE=topSE, topSEextra=topSEextra)\n",
    "    acc = screen_for_hits(df, posnegRat=posnegRat)\n",
    "    return df, acc\n",
    "    \n",
    "def gridSearch_mymethod(SEvocab):\n",
    "    params = {'top_cutoff': [5e-2],\n",
    "             'topRev': [25,50,75],\n",
    "             'topRevextra':[10,15,20],\n",
    "             'topSE': [3,5],\n",
    "             'topSEextra':[3,5],\n",
    "             'posnegRat':[6]}\n",
    "#     params = {'top_cutoff': [5e-3],\n",
    "#              'topRev': [50],#[25,50,75],\n",
    "#              'topRevextra':[20],#[10,15,20],\n",
    "#              'topSE': [5],#[3,5,10],\n",
    "#              'topSEextra':[5],#[1,3,5],\n",
    "#              'posnegRat':np.arange(3.5,10.2,0.5)}\n",
    "    \n",
    "    counts = 1\n",
    "    #total = 3*3*3*3*3*4\n",
    "    total = 3*4\n",
    "    score = []\n",
    "    for tc in params['top_cutoff']:\n",
    "        for tR in params['topRev']:\n",
    "            for tRe in params['topRevextra']:\n",
    "                for tSE in params['topSE']:\n",
    "                    for tSEe in params['topSEextra']:\n",
    "                        for i,pnR in enumerate(params['posnegRat']):\n",
    "                            if i == 0:\n",
    "                                df, acc = mymethod(SEvocab, top_cutoff=tc, topRev=tR, topRevextra=tRe,\n",
    "                                                  topSE=tSE, topSEextra=tSEe, posnegRat=pnR)\n",
    "                            else:\n",
    "                                acc = screen_for_hits(df, posnegRat=pnR)\n",
    "                            score.append([tc,tR,tRe,tSE,tSEe,pnR, acc])\n",
    "                            counts+=1\n",
    "                            print('progress:\\t{:g}\\n\\n'.format(round(counts/total,2)))\n",
    "                            \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitXGBoost(feat_train, labels_train, feat_test, labels_test, seed=616):\n",
    "    # Fit the model\n",
    "    clf = XGBClassifier(seed=seed)\n",
    "    clf.fit(feat_train, labels_train)\n",
    "    labels_predict = clf.predict(feat_test)\n",
    "    \n",
    "    # Getting ranked feature importance\n",
    "    feature_weights = clf.feature_importances_\n",
    "    print(\"Accuracy: \", accuracy_score(labels_test, labels_predict))\n",
    "    \n",
    "    return feature_weights, confusion_matrix(labels_test, labels_predict).ravel()\n",
    "\n",
    "def fitRF(feat_train, labels_train, feat_test, labels_test, \n",
    "          n_est=100, max_depth=10, seed=616):\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators=n_est,\n",
    "                                 max_depth=max_depth, \n",
    "                                 random_state=616,\n",
    "                                 max_samples=0.8)\n",
    "    clf.fit(feat_train, labels_train)\n",
    "    \n",
    "    feature_weights = clf.feature_importances_\n",
    "    labels_predict = clf.predict(feat_test)\n",
    "    \n",
    "    # Metrics\n",
    "    print(\"Accuracy: \", accuracy_score(labels_test, labels_predict))\n",
    "    \n",
    "    return feature_weights, confusion_matrix(labels_test, labels_predict).ravel()\n",
    "    \n",
    "def fitLR(feat_train, labels_train, feat_test, labels_test, seed=616):\n",
    "    clf = LogisticRegression(random_state=seed)\n",
    "    clf.fit(feat_train, labels_train)\n",
    "    feature_weights = clf.coef_\n",
    "    labels_predict = clf.predict(feat_test)\n",
    "    \n",
    "    print(\"Accuracy: \", accuracy_score(labels_test, labels_predict))\n",
    "    \n",
    "    return feature_weights, confusion_matrix(labels_test, labels_predict).ravel()\n",
    "    \n",
    "def fitKFolds(features, labels, fitter=None):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=4)\n",
    "    for train_index, test_index in kf.split(features):\n",
    "        x_train, x_test = features.loc[train_index], features.loc[test_index]\n",
    "        y_train, y_test = labels.loc[train_index], labels.loc[test_index]\n",
    "        dummy, confusion = fitter(x_train, y_train, x_test, y_test)\n",
    "        print(\"tn, fp, fn, tp\")\n",
    "        print(confusion)\n",
    "        \n",
    "def grid_search(features, labels, fitmtd, parameters):\n",
    "    mtd = fitmtd()\n",
    "    clf = GridSearchCV(mtd, parameters, verbose=0)\n",
    "    clf.fit(features, labels)\n",
    "    \n",
    "    best_fitter = clf.best_estimator_\n",
    "    best_params = pd.DataFrame(clf.cv_results_).loc[clf.best_index_]\n",
    "    \n",
    "    return best_fitter, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working on the data and comparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the labeled data\n",
    "df = pd.read_csv('LabeledReviews/randomlySelectedReviews.csv', sep='$', index_col=0)\n",
    "\n",
    "# Getting the side effect vocab\n",
    "SEvocab = processSideEffects()\n",
    "\n",
    "# Getting the features\n",
    "feature_df = processLabeledReviews(SEvocabParsed)\n",
    "\n",
    "# Writing a dataframe with that info\n",
    "feature_df.to_csv('LabeledAndTFIDFed.csv', sep='$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the parameters to explore, and the test/train split\n",
    "parametersLR = {'solver': ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "               'C': [10,1,0.1,0.01,0.01],\n",
    "               'random_state':[616]}\n",
    "\n",
    "parametersRF = {'n_estimators': [50,100,500],\n",
    "               'max_depth':[5,10,25],\n",
    "               'min_samples_split':[2,3,4,5],\n",
    "               'min_samples_leaf':[2,4,8,16,32],\n",
    "               'bootstrap': ['False'],\n",
    "               'random_state': [616]}\n",
    "\n",
    "parametersXGB = {'eta': [0.3],#[0.1,0.3,0.5],\n",
    "                'gamma': [0,1,10],#,100],\n",
    "                'max_depth': [25],#[5,10,25],\n",
    "                'lambda': [0.1,1,10],\n",
    "                'seed':[616]}\n",
    "\n",
    "features = np.array(feature_df.drop(columns=['Side effect label']))\n",
    "inds = [ind for ind, col in enumerate(features.T) if np.unique(col).size > 1]\n",
    "features = features[:,np.array(inds)]\n",
    "features = (features-features.min(axis=0))/features.ptp(axis=0)\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(features, \n",
    "                                                      feature_df['Side effect label'], \n",
    "                                                      test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit all the models and tune them. Not tuning my method to begin, because that's a \n",
    "# beast and I just want to know if it's worth doing so\n",
    "fitter, params = grid_search(x_train, y_train, LogisticRegression, parametersLR)\n",
    "y_pred = fitter.predict(x_valid)\n",
    "print(\"Accuracy: \", accuracy_score(y_valid, y_pred))\n",
    "print(\"tn, fp, fn, tp\")\n",
    "print(confusion_matrix(y_valid, y_pred).ravel())\n",
    "\n",
    "fitter, params = grid_search(x_train, y_train, RandomForestClassifier, parametersRF)\n",
    "y_pred = fitter.predict(x_valid)\n",
    "print(\"Accuracy: \", accuracy_score(y_valid, y_pred))\n",
    "print(\"tn, fp, fn, tp\")\n",
    "print(confusion_matrix(y_valid, y_pred).ravel())\n",
    "\n",
    "fitter, params = grid_search(x_train, y_train, XGBClassifier, parametersXGB)\n",
    "y_pred = fitter.predict(x_valid)\n",
    "print(\"Accuracy: \", accuracy_score(y_valid, y_pred))\n",
    "print(\"tn, fp, fn, tp\")\n",
    "print(confusion_matrix(y_valid, y_pred).ravel())\n",
    "\n",
    "# Use my model\n",
    "dummy = mymethod(SEvocab)\n",
    "\n",
    "# The final accuracies are: \n",
    "# LR = 0.6, RF = 0.68, XGB = 0.57, homebrew = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finished by grid searching my method, and found that I could improve the accuracy 0.06\n",
    "# without dramatically increasing the computational cost by changing a few params\n",
    "score = gridSearch_mymethod(SEvocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
