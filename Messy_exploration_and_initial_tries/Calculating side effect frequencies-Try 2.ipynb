{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook I will:\n",
    "* Go through and remove reviews that only have advertisements? (NOT AT THIS TIME)\n",
    "* Tokenize, lemmatize, remove stop words, and remove instances of words that only show up once that aren't special (words that indicate a condition, medication, side effect, or caregiver role)\n",
    "* Rejoin processed review into a string for BOW analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# Haven't decided whether I like nltk or spacy better yet\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "stops = stopwords.words('english')\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()\n",
    "\n",
    "# A method to process text in nltk:\n",
    "# https://pythonhealthcare.org/2018/12/14/101-pre-processing-data-tokenization-stemming-and-removal-of-stop-words/\n",
    "\n",
    "# same process in spacy\n",
    "# https://spacy.io/usage/linguistic-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Just don't have time to learn these right now\n",
    "#from sklearn.base import TransformerMixin\n",
    "#from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting stop words in spacy to not lose a bunch of negatives for the sentiment analysis\n",
    "for word in [u'nor',u'none',u'not',u'alone',u'no',u'never',u'cannot',u'always']:\n",
    "    nlp.vocab[word].is_stop = False\n",
    "nlp.vocab[u'thing'].is_stop = True\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working on processing text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacyTokenizer(s: str)-> list:\n",
    "    doc = tokenizer(s.lower().strip())\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if not token.is_stop and token.is_alpha and token.lemma_ != '-PRON-':\n",
    "            tokens.append(token.lemma_)\n",
    "        \n",
    "    return tokens\n",
    "\n",
    "def getSynonyms(word):\n",
    "    #https://www.geeksforgeeks.org/get-synonymsantonyms-nltk-wordnet-python/\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for l in syn.lemmas():\n",
    "            synonyms+= spacyTokenizer(l.name().replace('_', ' ').lower())\n",
    "           \n",
    "    return list(set(synonyms))\n",
    "\n",
    "# Setting a very basic metric for uniqueness to test a new way to quantify side effect presence\n",
    "def quantify_uniqueness(word):\n",
    "    syns = getSynonyms(word)\n",
    "    #print(syns)\n",
    "    return len(syns+[word])**-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    # https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "#     elif treebank_tag.startswith('NN'):\n",
    "#         return wordnet.ADJ # Considering ADJ_SET to be same as ADJ\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_PoS(word, pos=wordnet.NOUN):\n",
    "    return get_wordnet_pos(nltk.pos_tag([word])[0][1]) == pos\n",
    "\n",
    "\n",
    "# Working off of:\n",
    "# https://nlpforhackers.io/convert-words-between-forms/\n",
    "\n",
    "def convert_word_plus_synonyms(word, to_pos=wordnet.NOUN):\n",
    "    # Finding all synonyms in all the parts of speech\n",
    "    words = []\n",
    "    syns = wordnet.synsets(word)\n",
    "\n",
    "    # Chopping down to most common versions of words...this works for side effects more than words like 'cat'\n",
    "    if len(syns)%2 and (len(syns) != 1):\n",
    "        synList = syns[:len(syns)//2]\n",
    "    else:\n",
    "        synList = syns[:len(syns)//2+1]\n",
    "\n",
    "    # Finding all the forms of a word\n",
    "    for syn in synList:\n",
    "        for l in syn.lemmas():\n",
    "            form = l.derivationally_related_forms()\n",
    "            words.append(l.name())\n",
    "            for f in form:\n",
    "                words.append(f.name())\n",
    "                \n",
    "    # Getting all the unique words that match the desired part of speech\n",
    "    words = list(np.unique(words))\n",
    "    pos = nltk.pos_tag(words)\n",
    "    return_words = [word for word, word_pos in pos if get_wordnet_pos(word_pos)==to_pos]\n",
    "\n",
    "    # Getting around weirdness with somehow dropping PoS for original word if matches to_pos (e.g., with weight)\n",
    "    if get_wordnet_pos(nltk.pos_tag([word])[0][1]) == to_pos and word not in return_words: return_words.append(word)\n",
    "        \n",
    "    return return_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the count vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Parsing side effects\n",
    "def parseSideEffects_basic(file):\n",
    "    # Loading in the file I curated\n",
    "    sideEff = np.genfromtxt(file, delimiter='$', dtype=str)\n",
    "\n",
    "    cleanedSEs = []\n",
    "    uniqueness = []\n",
    "    for SE in sideEff:\n",
    "        if len(SE.strip().split(' ')) == 1:\n",
    "            cleanedSEs.append(' '.join(list(set(getSynonyms(SE.strip().lower())))))\n",
    "        else:\n",
    "            cleanedSEs.append(' '.join(list(set(spacyTokenizer(SE)))))\n",
    "        uniqueness.append([quantify_uniqueness(word) for word in spacyTokenizer(cleanedSEs[-1])])\n",
    "    \n",
    "    return cleanedSEs, uniqueness\n",
    "\n",
    "def parseSideEffects_advanced(file):\n",
    "    sideEff = np.genfromtxt(file, delimiter='$', dtype=str)\n",
    "    sideEffectBlocks = {}\n",
    "    for SE in sideEff:\n",
    "        SEsplit = spacyTokenizer(SE)\n",
    "        syns = []\n",
    "        for word in SEsplit:\n",
    "            convword = convert_word_plus_synonyms(word)\n",
    "            syns.append(convword)\n",
    "        sideEffectBlocks[SE] = syns\n",
    "    return sideEffectBlocks\n",
    "\n",
    "def parseReviewSentences_basic(file):\n",
    "    reviews = pd.read_csv(file, sep='$')['Comment']\n",
    "    \n",
    "    cleanedRevs = [[' '.join(spacyTokenizer(sent)) for sent in rev.split('.')] for rev in reviews]\n",
    "    return cleanedRevs\n",
    "\n",
    "def findSideEffects_advanced(seFile, revFile):\n",
    "    # Parsing side effects and reviews\n",
    "    cleanedSEs = parseSideEffects_advanced(seFile)\n",
    "    cleanedRevs = parseReviewSentences_basic(revFile)\n",
    "    \n",
    "    tracer = {}\n",
    "    for i, rev in enumerate(cleanedRevs):\n",
    "        tracer[i] = {}\n",
    "        for j, cSE in enumerate(cleanedSEs):\n",
    "            tracer[i][cSE] = np.zeros(len(cleanedSEs[cSE]))\n",
    "            for k,wordSyns in enumerate(cleanedSEs[cSE]):\n",
    "                if wordSyns:\n",
    "                    vectorizer.vocabulary = wordSyns\n",
    "                    X = vectorizer.fit_transform(rev).toarray()\n",
    "                    result = (X > 0).sum() > 0\n",
    "                    tracer[i][cSE][k] += result\n",
    "    \n",
    "    return tracer, cleanedSEs.keys(), cleanedRevs\n",
    "    \n",
    "def findSideEffects_basic(seFile, revFile):\n",
    "    # Parsing side effects and reviews\n",
    "    cleanedSEs, uniqSEs = parseSideEffects_basic(seFile)\n",
    "    cleanedRevs = parseReviewSentences_basic(revFile)\n",
    "    \n",
    "    \n",
    "    # For each sentence in each review\n",
    "    tracer = {}\n",
    "    for i,rev in enumerate(cleanedRevs):\n",
    "        tracer[i] = {}\n",
    "        for j,cSE in enumerate(cleanedSEs):\n",
    "            concern = cSE.strip().split(' ')\n",
    "            vectorizer.vocabulary = concern\n",
    "            X = vectorizer.fit_transform(rev).toarray()\n",
    "            result = (X*np.array(uniqSEs[j])[-1,None]).sum(axis=1)\n",
    "            tracer[i][cSE] = result\n",
    "            \n",
    "    return tracer, cleanedSEs, cleanedRevs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "findings, SEs, Revs = findSideEffects_advanced('SideEffects/ADHD_SideEffects_denormed.csv',\n",
    "                                           'ProcessedReviews/ADHD/Bupropion_ADHD_parsed_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['drug screw sleep take difficult', 'want fair shake keep', 'develope odd neurological tingle twitch finger', 'associate drug', 'horrible neurological condition rush hospital', 'hypokalemia low potassium', 'turn associate', 'business boyfriend month late take smoke drive hospital reason', ''] \n",
      " Twitching \n",
      "\n",
      "\n",
      "['med day feel like massive pain knee muscle ear sore swell dry increase heart feel like chemical taste mouth like swallow bleach feel invincible focus little well not worth feel like verge death', 'design drug company spend billion dollar garbage come plz'] \n",
      " dry mouth \n",
      "\n",
      "\n",
      "['med day feel like massive pain knee muscle ear sore swell dry increase heart feel like chemical taste mouth like swallow bleach feel invincible focus little well not worth feel like verge death', 'design drug company spend billion dollar garbage come plz'] \n",
      " sore throat pain \n",
      "\n",
      "\n",
      "['med day feel like massive pain knee muscle ear sore swell dry increase heart feel like chemical taste mouth like swallow bleach feel invincible focus little well not worth feel like verge death', 'design drug company spend billion dollar garbage come plz'] \n",
      " swelling or inflammation of the mouth \n",
      "\n",
      "\n",
      "['begin shortness heart headache blur spot vision', 'medication week point scare health', 'notice increase blood pressure', 'tonight barely catch breath heart beat hard think have heart attack', '', '', 'leave theater fresh air catch breath', 'evidently drug choice', ''] \n",
      " Blurred vision \n",
      "\n",
      "\n",
      "['anxiety problem combination adhd bupropion seriously help', 'take entire exception early year 5 grade', 'recently go stint try learn primarily begin have problematic panic attack period not stop cry', 'satisfy', 'concerta bupropion help adhd check', 'know review probably matt want insight people adhd start debate start medication', 'lot review people take good adhd', 'few present effect good alternative', ''] \n",
      " seeing or hearing hallucinations \n",
      "\n",
      "\n",
      "['start take medicine week ago', 'start hear buzz ear ring', 'begin feel like water', 'start feel dizzy', 'spill drink accidentally miss step walk stair', 'generally feel not grind', 'needless throw prescription', 'sensitive throat family history seizure history ear not medicine', 'think go lose hear today ring loud noise unbearable', 'say know cause symptom', 'like evenness feel', 'health problem medicine cause week far outwiegh benefit', ''] \n",
      " dizziness \n",
      "\n",
      "\n",
      "['wellbutrin xr month', 'help mood', 'feel like start get thing easy', 'somewhat effective help focus thing know', 'keep focus learn new thing challenge', 'morning feel like go bed', 'able thing relax watch tv', 'feel', 'able work take', 'anxiety talk people work', 'feel different', 'feel go away day', 'never feel uncomfortable', 'case feel let know start week go', 'normal way start', 'plus go mg no ill effect', ''] \n",
      " seeing or hearing hallucinations \n",
      "\n",
      "\n",
      "['day wellbutrin sr mg daily quit smoke completely no urge', 'drink caffinated drink feel alittle crazy do anymore', 'energy day completely exhaust dizzy', 'help quit drink day alone make worth', 'hard time sleep couple deal worth', '', '', 'dr', 'start wellbutrin xl mg daily morning feel foggy no focus', 'memory ring sensitivity light wear hat go sun skin sting like crazy hour', 'oh high hope work month', 'no weight loss mabie high dose', 'med month', ''] \n",
      " dizziness \n",
      "\n",
      "\n",
      "['day wellbutrin sr mg daily quit smoke completely no urge', 'drink caffinated drink feel alittle crazy do anymore', 'energy day completely exhaust dizzy', 'help quit drink day alone make worth', 'hard time sleep couple deal worth', '', '', 'dr', 'start wellbutrin xl mg daily morning feel foggy no focus', 'memory ring sensitivity light wear hat go sun skin sting like crazy hour', 'oh high hope work month', 'no weight loss mabie high dose', 'med month', ''] \n",
      " increased sensitivity of the eyes or skin to light \n",
      "\n",
      "\n",
      "['day wellbutrin sr mg daily quit smoke completely no urge', 'drink caffinated drink feel alittle crazy do anymore', 'energy day completely exhaust dizzy', 'help quit drink day alone make worth', 'hard time sleep couple deal worth', '', '', 'dr', 'start wellbutrin xl mg daily morning feel foggy no focus', 'memory ring sensitivity light wear hat go sun skin sting like crazy hour', 'oh high hope work month', 'no weight loss mabie high dose', 'med month', ''] \n",
      " weight loss \n",
      "\n",
      "\n",
      "['chest pain dry mouth effect help depression', ''] \n",
      " dry mouth \n",
      "\n",
      "\n",
      "['dose daily', 'not good add', 'mean day effect dry mouth pretty severe', 'aweful taste mouth like metallic taste', 'dry mouth inconvenient end little voice leave', 'canker sore tongue dry mouth', ''] \n",
      " dry mouth \n",
      "\n",
      "\n",
      "['2 week mg xl', 'rapid heart beat moment insomnia', 'xanax help sleep overall feel positivity focus', '', '', 'hope affect subside', 'suppress appetite', 'sex drive great', 'follow soon', 'little thing not crisis no feeling', '', 'go', 'recommend week completely work', ''] \n",
      " trouble with sleeping insomnia \n",
      "\n",
      "\n",
      "['work combination lamictal', 'issue hive occasion', ''] \n",
      " hives \n",
      "\n",
      "\n",
      "['life cease suicidal give functional attention give energy complete decrease appetite', 'stop take pill find starve', 'never generic pill speak doctor', 'google avoid literally cost life', ''] \n",
      " decreased appetite \n",
      "\n",
      "\n",
      "['life cease suicidal give functional attention give energy complete decrease appetite', 'stop take pill find starve', 'never generic pill speak doctor', 'google avoid literally cost life', ''] \n",
      " decreased or loss of appetite \n",
      "\n",
      "\n",
      "['see thing hear noise no feel nervous anxious', 'take day figure go away make', 'stop take', 'live adhd', ''] \n",
      " seeing or hearing hallucinations \n",
      "\n",
      "\n",
      "['afflict bear break wellbutrin year old control hyperactive behavior', 'begin withdraw normal school work regress grade level', 'memory affect dramatically', 'remove wellbutrin treatment hear similar story effect', 'begin slow recovery function high level', 'ability know leave tell time skill year severely affect continue struggle issue', ''] \n",
      " seeing or hearing hallucinations \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in findings:\n",
    "     for j in findings[key]:\n",
    "            if findings[key][j].sum() > (len(findings[key][j])//2):\n",
    "                print(Revs[key],'\\n', j, '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in findings:\n",
    "#     for j in findings[i]:\n",
    "#         if findings[i][j].sum() > 0.25: print(findings[i][j].sum(), j, '\\n', Revs[i], '\\n\\n')\n",
    "        \n",
    "scores = []\n",
    "revs = {}\n",
    "for i in findings:\n",
    "    revs['. '.join(Revs[i])] = []\n",
    "    for j in findings[i]:\n",
    "        score = findings[i][j].sum()/len(j.split(' '))**2\n",
    "        scores.append(score)\n",
    "        if score > 0.03: revs['. '.join(Revs[i])].append(j)\n",
    "        \n",
    "plt.hist(scores, bins=100)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rev in revs:\n",
    "    if revs[rev]:\n",
    "        print(rev, '\\n', revs[rev], '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getSynonyms('sleepiness')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of classical sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working on sentiment analysis\n",
    "# Starting source: https://www.datacamp.com/community/tutorials/simplifying-sentiment-analysis-python\n",
    "# Other source: https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/\n",
    "\n",
    "def comment_features(comment):\n",
    "    document = list(nltk.FreqDist(w for w in spacyTokenizer(comment)))\n",
    "    document_words = set(document)\n",
    "    \n",
    "    features = {}\n",
    "    for word in document:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "def dichotomize_satisfaction(score):\n",
    "    if score > 3:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'negative'\n",
    "\n",
    "featuresets = [(comment_features(comment),dichotomize_satisfaction(sat)) for comment, sat in zip(parsedDF['Comment'], parsedDF['Effectiveness'])] \n",
    "classifier = nltk.NaiveBayesClassifier.train(featuresets)\n",
    "classifier.show_most_informative_features(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
