{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook I will:\n",
    "* Go through and remove reviews that only have advertisements? (NOT AT THIS TIME)\n",
    "* Tokenize, lemmatize, remove stop words, and remove instances of words that only show up once that aren't special (words that indicate a condition, medication, side effect, or caregiver role)\n",
    "* Rejoin processed review into a string for BOW analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# Haven't decided whether I like nltk or spacy better yet\n",
    "import nltk\n",
    "from nltk.corpus import wordnet#, stopwords\n",
    "#stops = stopwords.words('english')\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()\n",
    "\n",
    "# Magical gensim module\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel, LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# A method to process text in nltk:\n",
    "# https://pythonhealthcare.org/2018/12/14/101-pre-processing-data-tokenization-stemming-and-removal-of-stop-words/\n",
    "\n",
    "# same process in spacy\n",
    "# https://spacy.io/usage/linguistic-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/13928155/spell-checker-for-python/48280566\n",
    "from autocorrect import Speller\n",
    "spell = Speller(lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting stop words in spacy to not lose a bunch of negatives for the sentiment analysis\n",
    "# for word in [u'nor',u'none',u'not',u'alone',u'no',u'never',u'cannot',u'always']:\n",
    "#     nlp.vocab[word].is_stop = False\n",
    "# nlp.vocab[u'thing'].is_stop = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working on processing text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    # https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "#     elif treebank_tag.startswith('NN'):\n",
    "#         return wordnet.ADJ # Considering ADJ_SET to be same as ADJ\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def check_PoS(word):\n",
    "    return get_wordnet_pos(nltk.pos_tag([word])[0][1])\n",
    "\n",
    "def useful_synonyms(word):\n",
    "    # Finding PoS of word\n",
    "    to_pos = check_PoS(word)\n",
    "    \n",
    "    # Finding all synonyms in all the parts of speech\n",
    "    words = []\n",
    "    syns = wordnet.synsets(word)\n",
    "\n",
    "    # Chopping down to most common versions of words...this works for side effects more than words like 'cat'\n",
    "    if len(syns) >= 2:\n",
    "        synList = syns[:2]\n",
    "    else:\n",
    "        synList = syns\n",
    "    #     if len(syns)%2 and (len(syns) != 1):\n",
    "#         synList = syns[:len(syns)//2]\n",
    "#     else:\n",
    "#         synList = syns[:len(syns)//2+1]\n",
    "\n",
    "    # Finding all the forms of a word\n",
    "    for syn in synList:\n",
    "        for l in syn.lemmas():\n",
    "            form = l.derivationally_related_forms()\n",
    "            words.append(l.name())\n",
    "            for f in form:\n",
    "                words.append(f.name())\n",
    "                \n",
    "    # Getting all the unique words that match the desired part of speech\n",
    "    words = list(np.unique(words))\n",
    "    pos = nltk.pos_tag(words)\n",
    "    return_words = [word.replace('_',' ') for word, word_pos in pos if get_wordnet_pos(word_pos)==to_pos]\n",
    "\n",
    "    # Getting around weirdness with somehow dropping PoS for original word if matches to_pos (e.g., with weight)\n",
    "    if get_wordnet_pos(nltk.pos_tag([word])[0][1]) == to_pos and word not in return_words: return_words.append(word)\n",
    "        \n",
    "    return return_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic tokenizer thing\n",
    "def spacyTokenizer(s: str)-> list:\n",
    "    doc = tokenizer(s.lower().strip())\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if not token.is_stop and token.is_alpha and token.lemma_ != '-PRON-':\n",
    "            tokens.append(token.lemma_)\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseRevnew(file, return_df=False):\n",
    "    reviews = pd.read_csv(file, sep='$')['Comment']\n",
    "    clean_reviews = [spacyTokenizer(rev.replace('/', ' ')) for rev in reviews]\n",
    "    cleaner_reviews = findTop(clean_reviews, 50)\n",
    "    cleaner_reviews = [[spell(word.lower()) for word in rev] for rev in cleaner_reviews]\n",
    "    \n",
    "    if return_df:\n",
    "        return cleaner_reviews, reviews\n",
    "    else:\n",
    "        return cleaner_reviews#consider, ignore\n",
    "\n",
    "def parseSEorig(file):\n",
    "    sideEff = np.genfromtxt(file, delimiter='$', dtype=str)\n",
    "    clean_SEs = [[spell(word) for word in spacyTokenizer(SE)] for SE in sideEff]\n",
    "    cleaner_reviews = findTop(clean_SEs, 3)\n",
    "    \n",
    "    return clean_SEs\n",
    "    \n",
    "def parseSE_FAERs(file, meds):\n",
    "    sideEff = pd.read_csv(file, sep='$').set_index('Concept ID').dropna(subset=['Percentage observed'])\n",
    "    sideEff = sideEff.fillna(value='')\n",
    "\n",
    "    # Looking for the medication name that has the side effects\n",
    "    meds_obs = sideEff.copy(deep=True)\n",
    "    meds_obs['Medications observed'] = [obs.split(', ') for obs in meds_obs['Medications observed']]\n",
    "    \n",
    "    medList = []\n",
    "    for obs in meds_obs['Medications observed']: medList += obs\n",
    "        \n",
    "    to_check = np.unique(medList)\n",
    "    \n",
    "    Found = False\n",
    "    for med in meds.lower().split(', '):\n",
    "        if med in to_check: \n",
    "            Found = True\n",
    "            break # Stop when I've found the name\n",
    "\n",
    "    sideEff = sideEff[[med in obs for obs in meds_obs['Medications observed']]]\n",
    "    \n",
    "    sideEff['Joined'] = sideEff['Definition'] + sideEff['Synonyms']\n",
    "    check_both = lambda combo: sum([c in nlp.vocab for c in combo.split(' ')]) == len(combo.split(' '))\n",
    "    sideEff['Joined'] = [', '.join([word for word in words.split(', ') if word.find('-') == -1 and check_both(word)]) for words in sideEff['Joined']]\n",
    "    clean_SEs = [list(set([spell(word) for word in spacyTokenizer(SE)])) for SE in sideEff['Joined']]\n",
    "    clean_SEs = [[word for word in SE if len(word) > 3] for SE in clean_SEs]\n",
    "    clean_SEs = findTop(clean_SEs,5)\n",
    "    \n",
    "#     ignore = [SE for SE in clean_SEs if len(SE) <= 2]\n",
    "#     consider = [SE for SE in clean_SEs if len(SE) > 2]\n",
    "    \n",
    "#     # Testing effect of just adding in more language to work with\n",
    "#     new_consider = []\n",
    "#     for chunk in consider:\n",
    "#         extended = []\n",
    "#         for w in chunk:\n",
    "#             extended += [s for s in useful_synonyms(w) if s.find('_') == -1]\n",
    "#         new_consider.append(extended)\n",
    "    \n",
    "    return clean_SEs#consider, ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF section\n",
    "https://buhrmann.github.io/tfidf-analysis.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not super useful to take the mean across columns, instead look at top 10 scoring words in each side effect\n",
    "def findTop(strList, keeptop=10):\n",
    "    tfidf_vectr = TfidfVectorizer()\n",
    "    corpus = [' '.join(SE) for SE in strList]\n",
    "    tfidf_score = tfidf_vectr.fit_transform(corpus).toarray()\n",
    "    features = np.array(tfidf_vectr.get_feature_names())\n",
    "    \n",
    "    words = []\n",
    "    for row in tfidf_score:\n",
    "        inds = row.argsort()[::-1][:keeptop]\n",
    "        row_words = []\n",
    "        for ind in inds:\n",
    "            if row[ind].round(2) != 0:\n",
    "                row_words.append(features[ind])\n",
    "                #print(features[ind],' '*(50-len(features[ind])), row[ind].round(2))\n",
    "        #print('\\n')\n",
    "        if not row_words:\n",
    "            row_words = list(features[inds][:5])\n",
    "        words.append(row_words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA/LDA section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python\n",
    "def genDictandDocMatrix(cleaned_text):\n",
    "    dictionary = corpora.Dictionary(cleaned_text)\n",
    "    matrix = [dictionary.doc2bow(doc) for doc in cleaned_text]\n",
    "    return dictionary, matrix\n",
    "\n",
    "def formatLSAresult(topics:list)->list:\n",
    "    for topic in topics:\n",
    "        title = \"Topic {:g}: \\n\".format(topic[0])\n",
    "        term_cluster = [term.strip().split('*')[1][1:-1] for term in topic[1].split('+')]\n",
    "        term_weight = [term.strip().split('*')[0] for term in topic[1].split('+')]\n",
    "\n",
    "        print(title, ', '.join(term_cluster),'\\n',', '.join(term_weight))\n",
    "        \n",
    "def produceLSA(n_topics, cleanText, n_word_report=10):\n",
    "    dictionary, matrix = genDictandDocMatrix(cleanText)\n",
    "    lsamodel = LsiModel(matrix, num_topics=n_topics, id2word=dictionary)\n",
    "    result = lsamodel.print_topics(num_topics=n_topics, num_words=n_word_report)\n",
    "\n",
    "    return result, lsamodel\n",
    "\n",
    "def produceLDA(n_topics, cleanText, n_word_report=10):\n",
    "    dictionary, matrix = genDictandDocMatrix(cleanText)\n",
    "    ldamodel = LdaModel(matrix, num_topics=n_topics, id2word=dictionary)\n",
    "    result = ldamodel.print_topics(num_topics=n_topics, num_words=n_word_report)\n",
    "\n",
    "    return result, ldamodel\n",
    "\n",
    "#result, model = produceLSA(10, reviews)\n",
    "#formatLSAresult(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing out idea of randomly joining side effects and pulling out concepts\n",
    "from random import shuffle\n",
    "\n",
    "#test = cleanSEs.copy()\n",
    "\n",
    "# Joining test results randomly\n",
    "def try_shuffled_LSA(test, numjoin=5):\n",
    "    joined_test = []\n",
    "    inds = np.arange(len(test))\n",
    "    shuffle(inds)\n",
    "\n",
    "    if inds.size % numjoin:\n",
    "        extras = inds[-(inds.size % numjoin):]\n",
    "        evendiv = inds[:-(inds.size % numjoin)]\n",
    "        inds = evendiv.reshape((-1,numjoin))\n",
    "    else:\n",
    "        extras = None\n",
    "        inds = inds.reshape((-1,numjoin))\n",
    "\n",
    "\n",
    "    for ind_set in inds:\n",
    "        new_join = []\n",
    "        for ind in ind_set: new_join += test[ind]\n",
    "        joined_test.append(new_join)\n",
    "\n",
    "    if type(extras) != type(None):\n",
    "        for i,ind in enumerate(extras):\n",
    "            joined_test[i] += test[ind]\n",
    "\n",
    "    result, model = produceLDA(len(test)//5, joined_test, 10)\n",
    "    formatLSAresult(result)\n",
    "#     topics = {}\n",
    "#      for topic in result:\n",
    "#         title = \"Topic {:g}: \\n\".format(topic[0])\n",
    "#         term_cluster = [term.strip().split('*')[1][1:-1] for term in topic[1].split('+')]\n",
    "#         term_weight = [term.strip().split('*')[0] for term in topic[1].split('+')]\n",
    "        \n",
    "#         topics[topic[0]] = term_cluster\n",
    "        \n",
    "#     return topics\n",
    "    \n",
    "    \n",
    "def process_shuffled_results(topic_dict_list):\n",
    "    for topics_dict in topic_dict_list:\n",
    "        word_pile = []\n",
    "        for key in topics_dict:\n",
    "            word_pile.append(topics_dict[key])\n",
    "        word_pile = np.array(word_pile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try_shuffled_LSA(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing some code to perform sentiment analysis on the full reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import vader\n",
    "VADER_SIA = vader.SentimentIntensityAnalyzer()\n",
    "\n",
    "def find_polarity_scores(reviews):\n",
    "    VADERscorePos = []\n",
    "    VADERscoreNeg = []\n",
    "    for rev in reviews:\n",
    "        VADERscorePos.append(VADER_SIA.polarity_scores(rev)['pos'])    \n",
    "        VADERscoreNeg.append(VADER_SIA.polarity_scores(rev)['neg'])            \n",
    "        \n",
    "    return VADERscorePos, VADERscoreNeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now checking for side effects in WebMD reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sideEffects_inReviews_FAERsinformed(revFile, sefile1, sefile2, faers=True):\n",
    "\n",
    "    # Parsing reviews\n",
    "    reviews, fullrevs = parseRevnew(revFile, return_df=True)\n",
    "    if faers:\n",
    "        cond = sefile1[sefile1.find('faers_results/')+14:sefile1.rfind('/')]\n",
    "        medication = revFile[revFile.rfind('/')+1:revFile.find('_'+cond)]\n",
    "        meds = pd.read_csv('UniqueMedications/Medications_unique_{:s}.csv'.format(cond), sep='$')['All names']\n",
    "        meds = [allnames for allnames in meds if medication.lower() in allnames.lower().split(', ')][0]\n",
    "\n",
    "    # Parsing side effects\n",
    "    if faers:\n",
    "        listSEs = parseSE_FAERs(sefile1, meds)\n",
    "    else:\n",
    "        listSEs = parseSEorig(sefile2)\n",
    "        \n",
    "    #new attempt\n",
    "    listSEs1 = parseSE_FAERs(sefile1, meds)\n",
    "    listSEs2 = parseSEorig(sefile2)\n",
    "    listSEs = listSEs1 + listSEs2\n",
    "    \n",
    "    BagOSE = ' '.join([' '.join(SE) for SE in listSEs])\n",
    "\n",
    "    # Finding review words that exist in the list of side effects\n",
    "    # Only requiring space at the beginning because of words like nausea-nauseated, etc.\n",
    "    found = [[word for word in rev if BagOSE.lower().find(' '+word.lower())] for rev in reviews]\n",
    "    found = []\n",
    "    for ind, rev in enumerate(reviews):\n",
    "        item = {}\n",
    "        for SE in listSEs:\n",
    "            # Match words in reviews to side effects and then add them to found, build dataframe with this info\n",
    "            item[', '.join(SE)] = len([word for word in rev if word.lower() in SE])\n",
    "        found.append(item)\n",
    "    \n",
    "    SE_match = pd.DataFrame(found)\n",
    "    SE_match['Full Review'] = fullrevs.values\n",
    "    \n",
    "    # Return the master product\n",
    "    return SE_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = find_sideEffects_inReviews_FAERsinformed('ProcessedReviews/Bipolar-Disorder/Lamictal_Bipolar-Disorder_parsed_reviews.csv', \n",
    "                                              'NERstuff/faers_results/Bipolar-Disorder/SideEffectsExtracted.csv',\n",
    "                                              'SideEffects/Bipolar-Disorder_SideEffects.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos, neg = find_polarity_scores(df['Full Review'])\n",
    "df['Positive polarity'] = pos\n",
    "df['Negative polarity'] = neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cond = (df['Negative polarity'] != 0)\n",
    "plt.hist(np.array(df['Positive polarity'][cond]/df['Negative polarity'][cond]), bins=100)\n",
    "plt.axvline(2.5)\n",
    "plt.yscale('log')\n",
    "\n",
    "diff_inds = df.index[cond][(df['Positive polarity'][cond]/df['Negative polarity'][cond] > 2.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = df.drop(columns=['Full Review', 'Positive polarity', 'Negative polarity'])\n",
    "review_inds = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def screen_for_hits(df):\n",
    "    newdf = df.drop(columns=['Full Review', 'Positive polarity', 'Negative polarity'])\n",
    "    review_inds = []\n",
    "    \n",
    "    # Allowing for two item side effects UNLESS they contain very generic words\n",
    "    colLens = np.array([len(col.split(', ')) + 2*((col.find('skin')!=-1)|\n",
    "                                                  (col.find('feel') != -1)|\n",
    "                                                  (col.find('pain')!= -1)|\n",
    "                                                  (col.find('abnormal')!=-1)|\n",
    "                                                  (col.find('change')!=-1)|\n",
    "                                                  (col.find('disorder')!=-1)|\n",
    "                                                  (col.find('problem')!=-1)|\n",
    "                                                  (col.find('decrease')!=-1)|\n",
    "                                                  (col.find('increase')!=-1)|\n",
    "                                                  (col.find('loss')!=-1)) for col in newdf.columns])\n",
    "    \n",
    "    # If the column is not generic and has two or fewer words, count one word as a match, otherwise require 2\n",
    "    for ind in newdf.index:\n",
    "        if (((colLens < 3) & newdf.loc[ind].gt(0)) | newdf.loc[ind].gt(1)).sum(): \n",
    "            review_inds.append(ind)\n",
    "            \n",
    "    # Screening based on polarity\n",
    "    cond = (df['Negative polarity'] != 0)\n",
    "    diff_inds = df.index[cond][(df['Positive polarity'][cond]/df['Negative polarity'][cond] > 2.5)]\n",
    "    \n",
    "    # Marking hits versus not\n",
    "    found_reviews = []\n",
    "    for ind in review_inds:\n",
    "        conditions = np.logical_or(np.logical_and((colLens < 3), newdf.loc[ind].gt(0)), newdf.loc[ind].gt(1))\n",
    "        newdf.loc[ind][conditions] = 1\n",
    "        newdf.loc[ind][not conditions] = 0\n",
    "        found_reviews.append(sum(conditions))\n",
    "    \n",
    "    # Creating one big dataframe\n",
    "    toconcat = df.drop(columns=[col for col in df.columns if col not in ['Full Review', 'Positive polarity', 'Negative polarity']])\n",
    "    master_df = pd.concat([toconcat, newdf], axis=1)\n",
    "    \n",
    "    # Not dropping rows with no hits because that is also important infomration\n",
    "    return master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Side effects match so now...\n",
    "* Only keep reviews that have side effects\n",
    "* Think on how to deal with meds with lower review counts\n",
    "* Process reviews medications with one hot encode\n",
    "* Perform sentiment analysis on each raw comment\n",
    "* Only count SEs if positive to negative sentiment is below some threshold\n",
    "* Add sentiments and encoded medications to the side effect DF, stack for each medication, and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
