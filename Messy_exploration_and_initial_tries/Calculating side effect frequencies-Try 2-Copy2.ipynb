{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook I will:\n",
    "* Go through and remove reviews that only have advertisements? (NOT AT THIS TIME)\n",
    "* Tokenize, lemmatize, remove stop words, and remove instances of words that only show up once that aren't special (words that indicate a condition, medication, side effect, or caregiver role)\n",
    "* Rejoin processed review into a string for BOW analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# Haven't decided whether I like nltk or spacy better yet\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet#, stopwords\n",
    "#stops = stopwords.words('english')\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()\n",
    "\n",
    "# Magical gensim module\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel, LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# A method to process text in nltk:\n",
    "# https://pythonhealthcare.org/2018/12/14/101-pre-processing-data-tokenization-stemming-and-removal-of-stop-words/\n",
    "\n",
    "# same process in spacy\n",
    "# https://spacy.io/usage/linguistic-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/13928155/spell-checker-for-python/48280566\n",
    "from autocorrect import Speller\n",
    "spell = Speller(lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting stop words in spacy to not lose a bunch of negatives for the sentiment analysis\n",
    "# for word in [u'nor',u'none',u'not',u'alone',u'no',u'never',u'cannot',u'always']:\n",
    "#     nlp.vocab[word].is_stop = False\n",
    "# nlp.vocab[u'thing'].is_stop = True\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working on processing text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    # https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "#     elif treebank_tag.startswith('NN'):\n",
    "#         return wordnet.ADJ # Considering ADJ_SET to be same as ADJ\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def check_PoS(word):\n",
    "    return get_wordnet_pos(nltk.pos_tag([word])[0][1])\n",
    "\n",
    "def useful_synonyms(word):\n",
    "    # Finding PoS of word\n",
    "    to_pos = check_PoS(word)\n",
    "    \n",
    "    # Finding all synonyms in all the parts of speech\n",
    "    words = []\n",
    "    syns = wordnet.synsets(word)\n",
    "\n",
    "    # Chopping down to most common versions of words...this works for side effects more than words like 'cat'\n",
    "    if len(syns) >= 2:\n",
    "        synList = syns[:2]\n",
    "    else:\n",
    "        synList = syns\n",
    "    #     if len(syns)%2 and (len(syns) != 1):\n",
    "#         synList = syns[:len(syns)//2]\n",
    "#     else:\n",
    "#         synList = syns[:len(syns)//2+1]\n",
    "\n",
    "    # Finding all the forms of a word\n",
    "    for syn in synList:\n",
    "        for l in syn.lemmas():\n",
    "            form = l.derivationally_related_forms()\n",
    "            words.append(l.name())\n",
    "            for f in form:\n",
    "                words.append(f.name())\n",
    "                \n",
    "    # Getting all the unique words that match the desired part of speech\n",
    "    words = list(np.unique(words))\n",
    "    pos = nltk.pos_tag(words)\n",
    "    return_words = [word.replace('_',' ') for word, word_pos in pos if get_wordnet_pos(word_pos)==to_pos]\n",
    "\n",
    "    # Getting around weirdness with somehow dropping PoS for original word if matches to_pos (e.g., with weight)\n",
    "    if get_wordnet_pos(nltk.pos_tag([word])[0][1]) == to_pos and word not in return_words: return_words.append(word)\n",
    "        \n",
    "    return return_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic tokenizer thing\n",
    "def spacyTokenizer(s: str)-> list:\n",
    "    doc = tokenizer(s.lower().strip())\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if not token.is_stop and token.is_alpha and token.lemma_ != '-PRON-':\n",
    "            tokens.append(token.lemma_)\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseRevnew(file):\n",
    "    sideEff = pd.read_csv(file, sep='$')['Comment']\n",
    "    clean_SEs = [spacyTokenizer(SE) for SE in sideEff]\n",
    "\n",
    "#     ignore = [SE for SE in clean_SEs if len(SE) <= 2]\n",
    "#     consider = [SE for SE in clean_SEs if len(SE) > 2]\n",
    "    \n",
    "#     # Testing effect of just adding in more language to work with\n",
    "#     new_consider = []\n",
    "#     for chunk in consider:\n",
    "#         extended = []\n",
    "#         for w in chunk:\n",
    "#             extended += [s for s in useful_synonyms(w) if s.find('_') == -1]\n",
    "#         new_consider.append(extended)\n",
    "    \n",
    "    return clean_SEs#consider, ignore\n",
    "\n",
    "def parseSEnew(file):\n",
    "    sideEff = np.genfromtxt(file, delimiter='$', dtype=str)\n",
    "    clean_SEs = [[spell(word) for word in spacyTokenizer(SE)] for SE in sideEff]\n",
    "\n",
    "#     ignore = [SE for SE in clean_SEs if len(SE) <= 2]\n",
    "#     consider = [SE for SE in clean_SEs if len(SE) > 2]\n",
    "    \n",
    "#     # Testing effect of just adding in more language to work with\n",
    "#     new_consider = []\n",
    "#     for chunk in consider:\n",
    "#         extended = []\n",
    "#         for w in chunk:\n",
    "#             extended += [s for s in useful_synonyms(w) if s.find('_') == -1]\n",
    "#         new_consider.append(extended)\n",
    "    \n",
    "    return clean_SEs#consider, ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python\n",
    "def genDictandDocMatrix(cleaned_text):\n",
    "    dictionary = corpora.Dictionary(cleaned_text)\n",
    "    matrix = [dictionary.doc2bow(doc) for doc in cleaned_text]\n",
    "    return dictionary, matrix\n",
    "\n",
    "def formatLSAresult(topics:list)->list:\n",
    "    for topic in topics:\n",
    "        title = \"Topic {:g}: \\n\".format(topic[0])\n",
    "        term_cluster = [term.strip().split('*')[1][1:-1] for term in topic[1].split('+')]\n",
    "        term_weight = [term.strip().split('*')[0] for term in topic[1].split('+')]\n",
    "\n",
    "        print(title, ', '.join(term_cluster),'\\n',', '.join(term_weight))\n",
    "        \n",
    "def produceLSA(n_topics, cleanText, n_word_report=10):\n",
    "    dictionary, matrix = genDictandDocMatrix(cleanText)\n",
    "    lsamodel = LsiModel(matrix, num_topics=n_topics, id2word=dictionary)\n",
    "    result = lsamodel.print_topics(num_topics=n_topics, num_words=n_word_report)\n",
    "\n",
    "    return result, lsamodel\n",
    "\n",
    "def produceLDA(n_topics, cleanText, n_word_report=10):\n",
    "    dictionary, matrix = genDictandDocMatrix(cleanText)\n",
    "    ldamodel = LdaModel(matrix, num_topics=n_topics, id2word=dictionary)\n",
    "    result = ldamodel.print_topics(num_topics=n_topics, num_words=n_word_report)\n",
    "\n",
    "    return result, ldamodel\n",
    "\n",
    "#result, model = produceLSA(10, reviews)\n",
    "#formatLSAresult(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sideEffects_inReviews(revFile, SEfile)\n",
    "\n",
    "    # Parsing reviews\n",
    "    reviews = parseRevnew(revFile)\n",
    "\n",
    "    # Finding TFIDF values for every review word\n",
    "    tfidf_vectr = TfidfVectorizer()\n",
    "    corpus = [' '.join(rev) for rev in reviews]\n",
    "    X = tfidf_vectr.fit_transform(corpus)\n",
    "\n",
    "    # Parsing side effects\n",
    "    listSEs = parseSEnew(SEfile)\n",
    "    BagOSE = ' '.join([' '.join(SE) for SE in listSEs])\n",
    "\n",
    "    # Finding review words that exist in the list of side effects\n",
    "    found = [(f, i) for i,f in enumerate(np.array(tfidf_vectr.get_feature_names())) if BagOSE.lower().find(' '+f+' ') != -1]\n",
    "    found = dict(found)\n",
    "    word_found = [f for f in found]\n",
    "\n",
    "    # Creating a tokenizer to drop words that were not found in the side effects\n",
    "    wordsNotFound = [f for f in tfidf_vectr.get_feature_names() if f not in word_found]\n",
    "    from spacy.vocab import Vocab\n",
    "    allWords = tfidf_vectr.get_feature_names()\n",
    "    vocab = Vocab(strings=allWords)\n",
    "\n",
    "    for word in allWords:\n",
    "        if word in wordsNotFound:\n",
    "            vocab[word].is_stop = True\n",
    "        else:\n",
    "            vocab[word].is_stop = False\n",
    "\n",
    "    SETokenizer = Tokenizer(vocab)\n",
    "\n",
    "    def SECleaner(s: str):\n",
    "        tokenized = SETokenizer(s)\n",
    "        tokens = []\n",
    "        for token in tokenized:\n",
    "            if not token.is_stop:\n",
    "                tokens.append(token.lemma_)\n",
    "\n",
    "        return list(set(tokens))\n",
    "\n",
    "    # Creating a processed corpus to only contain words present in the side effects list\n",
    "    processed_corpus = [SECleaner(rev) for rev in corpus]\n",
    "\n",
    "    # Scoring the level at which a review discusses side effects\n",
    "    scoreDict = {}\n",
    "    overall_score = X.toarray()\n",
    "    found_scores = dict([(word,overall_score[:,found[word]]) for word in found])\n",
    "\n",
    "    # Scoring each review\n",
    "    allscores = []\n",
    "    for SE in listSEs:\n",
    "        if SE:\n",
    "            key = ' '.join(SE)\n",
    "            scoreDict[key] = []\n",
    "            for i,result in enumerate(processed_corpus):\n",
    "                score = 0\n",
    "                for word in set(result):\n",
    "                    if len(word) > 1:\n",
    "                        TFIDF_score = found_scores[word][i]\n",
    "\n",
    "                        # I can't explain why this is true right now, but I will later\n",
    "                        if TFIDF_score: \n",
    "                            score += (word in SE)*TFIDF_score\n",
    "                                #print(TFIDF_score, word)\n",
    "                scoreDict[key].append(score)\n",
    "                allscores.append(score)\n",
    "\n",
    "    # Finding the upper decile of review scores\n",
    "    cutoff = np.percentile(allscores, 90)\n",
    "\n",
    "    # creating scored dataframe\n",
    "    score_dict_byrevs = []\n",
    "    for i in range(len(corpus)):\n",
    "        entry = {}\n",
    "        for key in scoreDict:\n",
    "            entry[key] = scoreDict[key][i]\n",
    "        score_dict_byrevs.append(entry)\n",
    "\n",
    "    df = pd.DataFrame(score_dict_byrevs)\n",
    "\n",
    "    # Return the master product\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
