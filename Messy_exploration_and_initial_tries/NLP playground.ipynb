{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook I will:\n",
    "* Go through and remove reviews that only have advertisements? (NOT AT THIS TIME)\n",
    "* Tokenize, lemmatize, remove stop words, and remove instances of words that only show up once that aren't special (words that indicate a condition, medication, side effect, or caregiver role)\n",
    "* Rejoin processed review into a string for BOW analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# Haven't decided whether I like nltk or spacy better yet\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet#, stopwords\n",
    "from nltk import sentiment\n",
    "VADER_SIA = sentiment.vader.SentimentIntensityAnalyzer()\n",
    "#stops = stopwords.words('english')\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()\n",
    "\n",
    "# Magical gensim module\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel, LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# A method to process text in nltk:\n",
    "# https://pythonhealthcare.org/2018/12/14/101-pre-processing-data-tokenization-stemming-and-removal-of-stop-words/\n",
    "\n",
    "# same process in spacy\n",
    "# https://spacy.io/usage/linguistic-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from scipy.spatial import distance\n",
    "cdist = distance.cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/13928155/spell-checker-for-python/48280566\n",
    "from autocorrect import Speller\n",
    "spell = Speller(lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting stop words in spacy to not lose a bunch of negatives for the sentiment analysis\n",
    "# for word in [u'nor',u'none',u'not',u'alone',u'no',u'never',u'cannot',u'always']:\n",
    "#     nlp.vocab[word].is_stop = False\n",
    "# nlp.vocab[u'thing'].is_stop = True\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working on processing text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    # https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "#     elif treebank_tag.startswith('NN'):\n",
    "#         return wordnet.ADJ # Considering ADJ_SET to be same as ADJ\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def check_PoS(word):\n",
    "    return get_wordnet_pos(nltk.pos_tag([word])[0][1])\n",
    "\n",
    "def useful_synonyms(word):\n",
    "    # Finding PoS of word\n",
    "    to_pos = check_PoS(word)\n",
    "    \n",
    "    # Finding all synonyms in all the parts of speech\n",
    "    words = []\n",
    "    syns = wordnet.synsets(word)\n",
    "\n",
    "    # Chopping down to most common versions of words...this works for side effects more than words like 'cat'\n",
    "    if len(syns) >= 2:\n",
    "        synList = syns[:2]\n",
    "    else:\n",
    "        synList = syns\n",
    "\n",
    "    # Finding all the forms of a word\n",
    "    for syn in synList:\n",
    "        for l in syn.lemmas():\n",
    "            form = l.derivationally_related_forms()\n",
    "            words.append(l.name())\n",
    "            for f in form:\n",
    "                words.append(f.name())\n",
    "                \n",
    "    # Getting all the unique words that match the desired part of speech\n",
    "    words = list(np.unique(words))\n",
    "    pos = nltk.pos_tag(words)\n",
    "    return_words = [word.replace('_',' ') for word, word_pos in pos if get_wordnet_pos(word_pos)==to_pos]\n",
    "\n",
    "    # Getting around weirdness with somehow dropping PoS for original word if matches to_pos (e.g., with weight)\n",
    "    if get_wordnet_pos(nltk.pos_tag([word])[0][1]) == to_pos and word not in return_words: return_words.append(word)\n",
    "        \n",
    "    return return_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic tokenizer thing\n",
    "def spacyTokenizer(s: str)-> list:\n",
    "    doc = tokenizer(s.lower().strip())\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if not token.is_stop and token.is_alpha and token.lemma_ != '-PRON-':\n",
    "            tokens.append(token.lemma_)\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bald', 'balding']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_synonyms('balding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
